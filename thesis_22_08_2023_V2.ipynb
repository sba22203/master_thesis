{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sba22203/master_thesis/blob/main/thesis_22_08_2023_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRYcJtXcTpnW"
      },
      "source": [
        "time series - prophet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dx20oBVpGwV"
      },
      "source": [
        "Now let's import all the libraries for our study\n",
        "Pandas: Data manipulation and analysis with DataFrames.\n",
        "NumPy:Numerical computing with arrays and math functions.\n",
        "Prophet: Time series forecasting tool by Facebook.\n",
        "Plotly:Interactive, publication-quality graphs and charts.\n",
        "Matplotlib: Versatile Python plotting library.\n",
        "PyStan:Python interface for Bayesian modeling with Stan.\n",
        "scikit-learn: Machine learning library for Python.\n",
        "NeuralProphet:Time series forecasting with deep learning.\n",
        "TensorFlow:Open-source machine learning framework by Google.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Vu2AqlR4ao",
        "outputId": "1a60f529-e808-4f87-ee03-06d29e764a9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: prophet in /usr/local/lib/python3.10/dist-packages (1.1.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting pystan\n",
            "  Downloading pystan-3.7.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting neuralprophet\n",
            "  Downloading neuralprophet-0.6.2-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Collecting arch\n",
            "  Downloading arch-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (916 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.4/916.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.1.0)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from prophet) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.30)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from prophet) (4.66.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet) (6.0.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.6 in /usr/local/lib/python3.10/dist-packages (from pystan) (3.8.5)\n",
            "Collecting clikit<0.7,>=0.6 (from pystan)\n",
            "  Downloading clikit-0.6.2-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpstan<4.11,>=4.10 (from pystan)\n",
            "  Downloading httpstan-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (44.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pysimdjson<6.0.0,>=5.0.2 (from pystan)\n",
            "  Downloading pysimdjson-5.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pystan) (67.7.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Collecting captum<0.7.0,>=0.6.0 (from neuralprophet)\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of neuralprophet to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting neuralprophet\n",
            "  Downloading neuralprophet-0.6.1-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neuralprophet-0.6.0-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading neuralprophet-0.5.4-py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from neuralprophet) (7.7.1)\n",
            "Collecting dash<2.9 (from neuralprophet)\n",
            "  Downloading dash-2.8.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly-resampler>=0.8.3 (from neuralprophet)\n",
            "  Downloading plotly_resampler-0.9.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning<2.0.0,>=1.7 (from neuralprophet)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from neuralprophet) (2.12.3)\n",
            "Collecting torch<2.0,>=1.8.0 (from neuralprophet)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.9.3 (from neuralprophet)\n",
            "  Downloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting widgetsnbextension>=4.0.5 (from neuralprophet)\n",
            "  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan) (1.3.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.1)\n",
            "Collecting crashtest<0.4.0,>=0.3.0 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading crashtest-0.3.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting pastel<0.3.0,>=0.2.0 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading pastel-0.2.1-py2.py3-none-any.whl (6.0 kB)\n",
            "Collecting pylev<2.0,>=1.3 (from clikit<0.7,>=0.6->pystan)\n",
            "  Downloading pylev-1.4.0-py2.py3-none-any.whl (6.1 kB)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->prophet) (0.5.12)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash<2.9->neuralprophet) (2.2.5)\n",
            "Collecting dash-html-components==2.0.0 (from dash<2.9->neuralprophet)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash<2.9->neuralprophet)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash<2.9->neuralprophet)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: appdirs<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from httpstan<4.11,>=4.10->pystan) (1.4.4)\n",
            "Collecting marshmallow<4.0,>=3.10 (from httpstan<4.11,>=4.10->pystan)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webargs<9.0,>=8.0 (from httpstan<4.11,>=4.10->pystan)\n",
            "  Downloading webargs-8.3.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->neuralprophet) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->neuralprophet) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->neuralprophet) (5.7.1)\n",
            "INFO: pip is looking at multiple versions of ipywidgets to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ipywidgets>=7.5.1 (from neuralprophet)\n",
            "  Downloading ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting comm>=0.1.3 (from ipywidgets>=7.5.1->neuralprophet)\n",
            "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->neuralprophet) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->neuralprophet) (3.0.8)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet) (4.1.4)\n",
            "INFO: pip is looking at multiple versions of plotly-resampler to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting plotly-resampler>=0.8.3 (from neuralprophet)\n",
            "  Downloading plotly_resampler-0.9.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading plotly_resampler-0.8.3.2-cp310-cp310-manylinux_2_35_x86_64.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupyter-dash>=0.4.2 (from plotly-resampler>=0.8.3->neuralprophet)\n",
            "  Downloading jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n",
            "Collecting orjson<4.0.0,>=3.8.0 (from plotly-resampler>=0.8.3->neuralprophet)\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trace-updater>=0.0.8 (from plotly-resampler>=0.8.3->neuralprophet)\n",
            "  Downloading trace_updater-0.0.9.1-py3-none-any.whl (185 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.2/185.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>=1.7->neuralprophet) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<2.0.0,>=1.7->neuralprophet) (2023.6.0)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<2.0.0,>=1.7->neuralprophet)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->neuralprophet) (2.3.7)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0,>=1.8.0->neuralprophet)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0,>=1.8.0->neuralprophet)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0,>=1.8.0->neuralprophet)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0,>=1.8.0->neuralprophet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.0.4->dash<2.9->neuralprophet) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.0.4->dash<2.9->neuralprophet) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.0.4->dash<2.9->neuralprophet) (8.1.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->neuralprophet) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->neuralprophet) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->neuralprophet) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->neuralprophet) (1.3.1)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (4.8.0)\n",
            "Collecting retrying (from jupyter-dash>=0.4.2->plotly-resampler>=0.8.3->neuralprophet)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Collecting ansi2html (from jupyter-dash>=0.4.2->plotly-resampler>=0.8.3->neuralprophet)\n",
            "  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from jupyter-dash>=0.4.2->plotly-resampler>=0.8.3->neuralprophet) (1.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->neuralprophet) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->neuralprophet) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->neuralprophet) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->neuralprophet) (2.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.5.1->neuralprophet) (0.2.6)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->neuralprophet) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->neuralprophet) (3.2.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->neuralprophet) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->neuralprophet) (6.3.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->neuralprophet) (5.3.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->neuralprophet) (23.2.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->neuralprophet) (3.10.0)\n",
            "Installing collected packages: trace-updater, pylev, dash-table, dash-html-components, dash-core-components, widgetsnbextension, retrying, pysimdjson, pastel, orjson, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, marshmallow, lightning-utilities, jedi, crashtest, comm, ansi2html, webargs, nvidia-cudnn-cu11, clikit, torch, ipywidgets, httpstan, dash, torchmetrics, pystan, jupyter-dash, captum, arch, pytorch-lightning, plotly-resampler, neuralprophet\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.5\n",
            "    Uninstalling widgetsnbextension-3.6.5:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ansi2html-1.8.0 arch-6.1.0 captum-0.6.0 clikit-0.6.2 comm-0.1.4 crashtest-0.3.1 dash-2.8.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 httpstan-4.10.1 ipywidgets-8.1.0 jedi-0.19.0 jupyter-dash-0.4.2 lightning-utilities-0.9.0 marshmallow-3.20.1 neuralprophet-0.5.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 orjson-3.9.5 pastel-0.2.1 plotly-resampler-0.8.3.2 pylev-1.4.0 pysimdjson-5.0.2 pystan-3.7.0 pytorch-lightning-1.9.5 retrying-1.3.4 torch-1.13.1 torchmetrics-1.0.3 trace-updater-0.0.9.1 webargs-8.3.0 widgetsnbextension-4.0.8\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy prophet plotly matplotlib pystan scikit-learn neuralprophet tensorflow arch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xtcCoTuBWFf_",
        "outputId": "2b3a3bcb-4da6-4226-c19c-439a441d5bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: prophet in /usr/local/lib/python3.10/dist-packages (1.1.4)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting torch\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.23.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from prophet) (3.7.1)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet) (1.5.3)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from prophet) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet) (0.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from prophet) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from prophet) (4.66.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.99)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch) (11.10.3.66)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->prophet) (0.5.12)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet) (4.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->prophet) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->prophet) (3.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->prophet) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-cupti-cu11, nvidia-cusolver-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "neuralprophet 0.5.4 requires torch<2.0,>=1.8.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cuda-cupti-cu11-11.7.101 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade prophet torchaudio torchdata torchtext torchvision torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxWFf_EUqEcV"
      },
      "source": [
        "Now, let's import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CdzFt0ONSRT_"
      },
      "outputs": [],
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import statsmodels.api as sm\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots  # Import make_subplots\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from arch import arch_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, auc, precision_recall_curve\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from prophet import Prophet\n",
        "from sklearn.svm import SVC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqXvZpIC8jQF"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVkLA_9Z8tlk"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqwadFDwswrl"
      },
      "source": [
        "Now, let' s import several indexes from Euronext and Finance yahoo (2 years 11/08/2021 to 11/08/2023) BEL20 (belgium), FTSE100 (UK), CAC40, ISEQ20 (ireland), DAX40 Germany), PSI20 (portugal), in order to preprocess them and compra the irish market to most important and relevant european indexes. And let's use date and close columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7hxaXpdDt4LC"
      },
      "outputs": [],
      "source": [
        "# List of dataset filenames\n",
        "datasets = [\n",
        "    \"BEL20.xlsx\",\n",
        "    \"FTSE100.xlsx\",\n",
        "    \"CAC40.xlsx\",\n",
        "    \"ISEQ20.xlsx\",\n",
        "    \"DAX40.xlsx\",\n",
        "    \"PSI20.xlsx\"\n",
        "]\n",
        "\n",
        "# Load datasets and calculate missing values and outliers\n",
        "for dataset in datasets:\n",
        "    df = pd.read_excel(dataset)\n",
        "    close_column = df['Close']\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RPmgul2F5abJ",
        "outputId": "9a311574-c0ee-4f4e-d493-67780212b48b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Basic Statistics for 'BEL20.xlsx' - 'Close' Column:\n",
            "count     517.000000\n",
            "mean     3885.008603\n",
            "std       259.889928\n",
            "min      3313.820068\n",
            "25%      3685.550049\n",
            "50%      3846.550049\n",
            "75%      4125.850098\n",
            "max      4402.319824\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n",
            "Basic Statistics for 'FTSE100.xlsx' - 'Close' Column:\n",
            "count     523.000000\n",
            "mean     7286.048107\n",
            "std      1050.270680\n",
            "min         0.000000\n",
            "25%      7218.675000\n",
            "50%      7452.840000\n",
            "75%      7588.340000\n",
            "max      8014.310000\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n",
            "Basic Statistics for 'CAC40.xlsx' - 'Close' Column:\n",
            "count     517.000000\n",
            "mean     6758.133625\n",
            "std       460.058401\n",
            "min      5676.870117\n",
            "25%      6449.379883\n",
            "50%      6731.370117\n",
            "75%      7152.600098\n",
            "max      7577.000000\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n",
            "Basic Statistics for 'ISEQ20.xlsx' - 'Close' Column:\n",
            "count     510.000000\n",
            "mean     1346.220976\n",
            "std       136.906465\n",
            "min      1063.239990\n",
            "25%      1227.999970\n",
            "50%      1381.984985\n",
            "75%      1469.257507\n",
            "max      1545.069946\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n",
            "Basic Statistics for 'DAX40.xlsx' - 'Close' Column:\n",
            "count      513.000000\n",
            "mean     14759.115925\n",
            "std       1150.595371\n",
            "min      11975.549805\n",
            "25%      13914.070313\n",
            "50%      15134.040039\n",
            "75%      15790.339844\n",
            "max      16469.750000\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n",
            "Basic Statistics for 'PSI20.xlsx' - 'Close' Column:\n",
            "count     516.000000\n",
            "mean     5812.663099\n",
            "std       265.054994\n",
            "min      5190.279785\n",
            "25%      5605.744873\n",
            "50%      5862.504882\n",
            "75%      6016.569824\n",
            "max      6349.209961\n",
            "Name: Close, dtype: float64\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# print basic statistics\n",
        "for dataset in datasets:\n",
        "    df = pd.read_excel(dataset)\n",
        "    close_column = df['Close']\n",
        "\n",
        "    # Print basic statistics for the 'Close' column\n",
        "    print(f\"Basic Statistics for '{dataset}' - 'Close' Column:\")\n",
        "    print(close_column.describe())\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbxKBEGUwMbN"
      },
      "source": [
        "Missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1qD0qaC83LE-",
        "outputId": "dd342bbf-8e77-4f04-e013-6e2f5bb8db06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values in 'BEL20.xlsx' - 'Close' Column: 0\n",
            "Missing Values in 'FTSE100.xlsx' - 'Close' Column: 0\n",
            "Missing Values in 'CAC40.xlsx' - 'Close' Column: 0\n",
            "Missing Values in 'ISEQ20.xlsx' - 'Close' Column: 0\n",
            "Missing Values in 'DAX40.xlsx' - 'Close' Column: 0\n",
            "Missing Values in 'PSI20.xlsx' - 'Close' Column: 0\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate and print missing values\n",
        "def calculate_missing_values(dataset_name, column_name):\n",
        "    missing_values = close_column.isnull().sum()\n",
        "    print(f\"Missing Values in '{dataset_name}' - '{column_name}' Column: {missing_values}\")\n",
        "\n",
        "# Calculate and print missing values for each dataset\n",
        "for dataset in datasets:\n",
        "    calculate_missing_values(dataset, 'Close')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJqj_UbiwQDl"
      },
      "source": [
        "Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xwXLGseqwPS1",
        "outputId": "bed7cde8-c803-40d1-93f1-4ddb5393e234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Potential Outliers in Close column 'BEL20.xlsx\n",
            "Empty DataFrame\n",
            "Columns: [Date, Close]\n",
            "Index: []\n",
            "Potential Outliers in Close column 'FTSE100.xlsx\n",
            "          Date  Close\n",
            "159 2023-01-02    0.0\n",
            "160 2022-12-30    0.0\n",
            "163 2022-12-27    0.0\n",
            "164 2022-12-26    0.0\n",
            "165 2022-12-23    0.0\n",
            "419 2022-01-03    0.0\n",
            "420 2021-12-31    0.0\n",
            "423 2021-12-28    0.0\n",
            "424 2021-12-27    0.0\n",
            "425 2021-12-24    0.0\n",
            "Potential Outliers in Close column 'CAC40.xlsx\n",
            "Empty DataFrame\n",
            "Columns: [Date, Close]\n",
            "Index: []\n",
            "Potential Outliers in Close column 'ISEQ20.xlsx\n",
            "Empty DataFrame\n",
            "Columns: [Date, Close]\n",
            "Index: []\n",
            "Potential Outliers in Close column 'DAX40.xlsx\n",
            "Empty DataFrame\n",
            "Columns: [Date, Close]\n",
            "Index: []\n",
            "Potential Outliers in Close column 'PSI20.xlsx\n",
            "Empty DataFrame\n",
            "Columns: [Date, Close]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Function to identify and print outliers using Z-score\n",
        "def identify_and_print_outliers(dataset_name, z_threshold=3.0):\n",
        "    df = pd.read_excel(dataset)\n",
        "    close_column = df['Close']\n",
        "    # Extract 'Date' and 'Close' columns\n",
        "    date_column = df['Date']\n",
        "\n",
        "    # Calculate Z-scores for the 'Close' column\n",
        "    z_scores = np.abs((close_column - close_column.mean()) / close_column.std())\n",
        "\n",
        "    # Identify and print outliers\n",
        "    outliers = df[z_scores > z_threshold]\n",
        "    print(f\"Potential Outliers in Close column '{dataset_name}\")\n",
        "    print(outliers[['Date', 'Close']])\n",
        "\n",
        "# Set the Z-score threshold for identifying outliers\n",
        "z_threshold = 3.0\n",
        "\n",
        "# Identify and print outliers for each dataset in the 'Close' column\n",
        "for dataset in datasets:\n",
        "    identify_and_print_outliers(dataset, z_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6NQvJFwHE9gV",
        "outputId": "18796205-2398-4460-dfab-bbe0999cee95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outliers removed from 'FTSE100.xlsx' and cleaned data saved to 'Cleaned_FTSE100.xlsx'\n",
            "Cleaned data saved to 'Cleaned_BEL20.xlsx'\n",
            "Cleaned data saved to 'Cleaned_CAC40.xlsx'\n",
            "Cleaned data saved to 'Cleaned_ISEQ20.xlsx'\n",
            "Cleaned data saved to 'Cleaned_DAX40.xlsx'\n",
            "Cleaned data saved to 'Cleaned_PSI20.xlsx'\n"
          ]
        }
      ],
      "source": [
        "# Function to identify and remove outliers using Z-score\n",
        "def remove_outliers(dataset_name, z_threshold=3.0):\n",
        "    df = pd.read_excel(dataset_name)\n",
        "    close_column = df['Close']\n",
        "\n",
        "    # Calculate Z-scores for the 'Close' column\n",
        "    z_scores = np.abs((close_column - close_column.mean()) / close_column.std())\n",
        "\n",
        "    # Identify and remove outliers\n",
        "    df_cleaned = df[z_scores <= z_threshold]\n",
        "    return df_cleaned\n",
        "\n",
        "# Set the Z-score threshold for identifying outliers\n",
        "z_threshold = 3.0\n",
        "\n",
        "# Remove outliers and change names for 'FTSE100_2years.xlsx'\n",
        "ftse_dataset = \"FTSE100.xlsx\"\n",
        "cleaned_ftse_data = remove_outliers(ftse_dataset, z_threshold)\n",
        "cleaned_ftse_dataset_name = \"Cleaned_\" + ftse_dataset\n",
        "cleaned_ftse_data.to_excel(cleaned_ftse_dataset_name, index=False)\n",
        "print(f\"Outliers removed from '{ftse_dataset}' and cleaned data saved to '{cleaned_ftse_dataset_name}'\")\n",
        "\n",
        "# Change names for other datasets\n",
        "for dataset in datasets:\n",
        "    if dataset != ftse_dataset:\n",
        "        new_dataset_name = \"Cleaned_\" + dataset\n",
        "        pd.read_excel(dataset).to_excel(new_dataset_name, index=False)\n",
        "        print(f\"Cleaned data saved to '{new_dataset_name}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBk8rh0vBGMS"
      },
      "source": [
        "#**comparision ISEQ20 to other indices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExE8EuSUeF7"
      },
      "source": [
        "After cleaning the data, let's compare the european indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7C1OBb-bBlTD",
        "outputId": "0d9e69b7-9a1f-428e-bd2a-5e4e5d175abe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4361b9fc745c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# comining datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataframes' is not defined"
          ]
        }
      ],
      "source": [
        "# comining datasets\n",
        "combined_df = pd.concat([df.set_index('Date')['Close'] for df in dataframes], axis=1)\n",
        "combined_df.columns = datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i4kp7N_VCB1q"
      },
      "outputs": [],
      "source": [
        "fig = px.line(combined_df, x=combined_df.index, y=combined_df.columns, title='Close Prices of european Indices')\n",
        "fig.update_xaxes(title_text='Date')\n",
        "fig.update_yaxes(title_text='Close Price')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYQy3WIQUyTz"
      },
      "source": [
        "let's create a heatmap and see what are the correlations between indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W4s6HDAoCsmR"
      },
      "outputs": [],
      "source": [
        "# Calculate correlation matrix\n",
        "correlation_matrix = combined_df.corr()\n",
        "\n",
        "# Create a heatmap using Seaborn\n",
        "plt.figure(figsize=(9, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0)\n",
        "plt.title('Correlation Heatmap of Close Prices')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FVsXMa5ODHJf"
      },
      "outputs": [],
      "source": [
        "# Load the datasets for CAC 40 and ISEQ 20\n",
        "dax_data = pd.read_excel(\"Cleaned_DAX40.xlsx\")\n",
        "iseq20_data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Merge the two datasets based on the Date column\n",
        "merged_data = pd.merge(dax_data, iseq20_data, on=\"Date\", how=\"inner\", suffixes=(\"_DAX40\", \"_ISEQ20\"))\n",
        "\n",
        "# Define a threshold for meeting points\n",
        "threshold = 10  # You can adjust this value\n",
        "merged_data[\"Price_Difference\"] = abs(merged_data[\"Close_DAX40\"] - merged_data[\"Close_ISEQ20\"])\n",
        "meeting_points = merged_data[merged_data[\"Price_Difference\"] < threshold]\n",
        "\n",
        "# Create the scatter plot\n",
        "fig = px.scatter(\n",
        "    merged_data,\n",
        "    x=\"Close_DAX40\",\n",
        "    y=\"Close_ISEQ20\",\n",
        "    title=\"Scatter Plot: DAX 40 vs ISEQ 20\",\n",
        "    color=\"Price_Difference\",\n",
        "    color_continuous_scale=\"Viridis\",  # Use a built-in color scale\n",
        "    labels={\"Close_DAX40\": \"DAX40 Close Price\", \"Close_ISEQ20\": \"ISEQ 20 Close Price\"},\n",
        "    hover_name=\"Date\"\n",
        ")\n",
        "\n",
        "# Add meeting points as annotations\n",
        "for index, row in meeting_points.iterrows():\n",
        "    fig.add_annotation(\n",
        "        x=row[\"Close_DAX\"],\n",
        "        y=row[\"Close_ISEQ20\"],\n",
        "        text=\"Meeting Point\",\n",
        "        showarrow=True,\n",
        "        arrowhead=2,\n",
        "        arrowcolor=\"red\",\n",
        "    )\n",
        "\n",
        "# Customize plot appearance\n",
        "fig.update_layout(\n",
        "    coloraxis_colorbar=dict(title=\"Price Difference\"),\n",
        "    coloraxis_colorbar_len=0.6,\n",
        "    coloraxis_colorbar_x=0.95,\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjkHa6BCfk1Y"
      },
      "source": [
        "let's compare bot indexes, and we can see they are both correlated and same evolution over time (when CAC goes down, iseq follows the same evolution, and when CAC is up, iseq goes up). CAC is a bigger index, of course compare to iseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IyMORGOle813"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the datasets for CAC 40 and ISEQ 20\n",
        "dax_df = pd.read_excel(\"Cleaned_DAX40.xlsx\")\n",
        "iseq20_df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Create a DataFrame for each index with Date and Close columns\n",
        "dax_data = pd.DataFrame({\"Date\": dax_df[\"Date\"], \"CAC 40\": dax_df[\"Close\"]})\n",
        "iseq20_data = pd.DataFrame({\"Date\": iseq20_df[\"Date\"], \"ISEQ 20\": iseq20_df[\"Close\"]})\n",
        "\n",
        "# Merge the dataframes based on the Date column\n",
        "merged_df = pd.merge(dax_data, iseq20_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Create a stacked area chart using Plotly Express\n",
        "fig = px.area(merged_df, x=\"Date\", y=[\"CAC 40\", \"ISEQ 20\"],\n",
        "              title=\"Comparison of CAC 40 and ISEQ 20 Evolution\",\n",
        "              labels={\"Date\": \"Date\", \"value\": \"Close Price\"},\n",
        "              template=\"plotly_dark\",\n",
        "              color_discrete_sequence=['#1f77b4', '#ff7f0e'])\n",
        "\n",
        "# Adjust the trace opacity\n",
        "fig.update_traces(opacity=0.7)\n",
        "\n",
        "fig.update_layout(legend_title_text=\"Index\")\n",
        "fig.update_xaxes(type=\"date\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-CzJRWHJSZ3J"
      },
      "outputs": [],
      "source": [
        "# Load the datasets for CAC 40 and ISEQ 20\n",
        "dax_df = pd.read_excel(\"Cleaned_DAX40.xlsx\")\n",
        "iseq20_df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Create a DataFrame with Date and Close columns for each index\n",
        "dax_data = pd.DataFrame({\"Date\": dax_df[\"Date\"], \"Close_DAX\": dax_df[\"Close\"]})\n",
        "iseq20_data = pd.DataFrame({\"Date\": iseq20_df[\"Date\"], \"Close_ISEQ20\": iseq20_df[\"Close\"]})\n",
        "\n",
        "# Merge the dataframes based on the Date column\n",
        "merged_df = pd.merge(dax_data, iseq20_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Calculate daily returns\n",
        "merged_df[\"Daily_Return_DAX\"] = merged_df[\"Close_DAX\"].pct_change()\n",
        "merged_df[\"Daily_Return_ISEQ20\"] = merged_df[\"Close_ISEQ20\"].pct_change()\n",
        "\n",
        "# Create a new column for color based on positive/negative returns\n",
        "merged_df[\"Color\"] = np.where(merged_df[\"Daily_Return_DAX\"] >= 0, \"Positive\", \"Negative\")\n",
        "\n",
        "# Create a subplot with two histograms for daily returns\n",
        "fig = px.histogram(merged_df, x=[\"Daily_Return_DAX\", \"Daily_Return_ISEQ20\"],\n",
        "                   nbins=30, opacity=0.7,\n",
        "                   facet_col=\"Color\", # Separate histograms by color\n",
        "                   labels={\"value\": \"Daily Return\", \"variable\": \"Index\"},\n",
        "                   title=\"Distribution of Daily Returns: DAX vs ISEQ 20\")\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    xaxis_title=\"Daily Return\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    bargap=0.1\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7hrxafXxVeJi"
      },
      "outputs": [],
      "source": [
        "# Load the datasets for CAC 40 and ISEQ 20\n",
        "dax_df = pd.read_excel(\"Cleaned_DAX40.xlsx\")\n",
        "iseq20_df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Create a DataFrame with Date and Close columns for each index\n",
        "Dax_data = pd.DataFrame({\"Date\": dax_df[\"Date\"], \"Close_DAX\": dax_df[\"Close\"]})\n",
        "iseq20_data = pd.DataFrame({\"Date\": iseq20_df[\"Date\"], \"Close_ISEQ20\": iseq20_df[\"Close\"]})\n",
        "\n",
        "# Merge the dataframes based on the Date column\n",
        "merged_df = pd.merge(dax_data, iseq20_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Calculate daily returns\n",
        "merged_df[\"Daily_Return_DAX\"] = merged_df[\"Close_DAX\"].pct_change()\n",
        "merged_df[\"Daily_Return_ISEQ20\"] = merged_df[\"Close_ISEQ20\"].pct_change()\n",
        "\n",
        "# Create a new column for color based on positive/negative returns\n",
        "merged_df[\"Color\"] = np.where(merged_df[\"Daily_Return_DAX\"] >= 0, \"Positive\", \"Negative\")\n",
        "\n",
        "# Create the scatter plot with regression line and colored points\n",
        "fig = px.scatter(merged_df, x=\"Daily_Return_DAX\", y=\"Daily_Return_ISEQ20\",\n",
        "                 color=\"Color\", # Use the \"Color\" column for coloring\n",
        "                 title=\"Scatter Plot of Daily Returns: DAX vs ISEQ 20\",\n",
        "                 trendline=\"ols\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jph4x8JiT2ve"
      },
      "outputs": [],
      "source": [
        "# Load the datasets for CAC 40 and ISEQ 20\n",
        "dax_df = pd.read_excel(\"Cleaned_DAX40.xlsx\")\n",
        "iseq20_df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Create a DataFrame with Date and Close columns for each index\n",
        "dax_data = pd.DataFrame({\"Date\": dax_df[\"Date\"], \"Close_DAX\": dax_df[\"Close\"]})\n",
        "iseq20_data = pd.DataFrame({\"Date\": iseq20_df[\"Date\"], \"Close_ISEQ20\": iseq20_df[\"Close\"]})\n",
        "\n",
        "# Merge the dataframes based on the Date column\n",
        "merged_df = pd.merge(dax_data, iseq20_data, on=\"Date\", how=\"inner\")\n",
        "\n",
        "# Calculate cumulative returns\n",
        "merged_df[\"Cumulative_Return_DAX\"] = (1 + merged_df[\"Close_DAX\"].pct_change()).cumprod() - 1\n",
        "merged_df[\"Cumulative_Return_ISEQ20\"] = (1 + merged_df[\"Close_ISEQ20\"].pct_change()).cumprod() - 1\n",
        "\n",
        "# Create cumulative return plots\n",
        "trace1 = go.Scatter(x=merged_df[\"Date\"], y=merged_df[\"Cumulative_Return_DAX\"],\n",
        "                    mode=\"lines\", name=\"DAX Cumulative Return\")\n",
        "trace2 = go.Scatter(x=merged_df[\"Date\"], y=merged_df[\"Cumulative_Return_ISEQ20\"],\n",
        "                    mode=\"lines\", name=\"ISEQ 20 Cumulative Return\")\n",
        "\n",
        "layout = go.Layout(title=\"Cumulative Returns: DAX vs ISEQ 20\",\n",
        "                   xaxis=dict(title=\"Date\"),\n",
        "                   yaxis=dict(title=\"Cumulative Return\"))\n",
        "\n",
        "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlK81ilBwiNa"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tlfw2USc7r8E"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file and process the data\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "fig = px.line(data, x='Date', y='Close', title='ISEQ 20 Closing Prices from 11/08/2021 to 11/08/2023', labels={'ds': 'Date', 'y': 'Closing Price'})\n",
        "fig.update_layout(showlegend=True, xaxis=dict(showgrid=True), yaxis=dict(showgrid=True))\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nJKXrJjS8jMU"
      },
      "outputs": [],
      "source": [
        "# Load the Excel file and process the data\n",
        "df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "df[\"Month\"] = df[\"Date\"].dt.month\n",
        "df[\"Year\"] = df[\"Date\"].dt.year\n",
        "\n",
        "# Reshape the DataFrame using pd.melt()\n",
        "melted_df = pd.melt(df, id_vars=[\"Month\"], value_vars=[\"Close\"], value_name=\"Close Price\")\n",
        "\n",
        "# Create the box plot\n",
        "box_plot = px.box(melted_df, x=\"Month\", y=\"Close Price\",\n",
        "                  labels={\"Month\": \"Month\", \"Close Price\": \"Close Price\"},\n",
        "                  title=\"Box Plots: Monthly Close Prices of ISEQ 20\")\n",
        "\n",
        "# Show the plot\n",
        "box_plot.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rLedLmZ9h6qt"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Create a pandas DataFrame\n",
        "data = pd.DataFrame({'ISEQ20': data['Close']})\n",
        "\n",
        "# Assuming your data has a datetime index\n",
        "n_points = len(data)\n",
        "data.index = pd.date_range(start='2021-08-11', periods=n_points, freq='D')\n",
        "\n",
        "# Perform decomposition\n",
        "iseq20_decomposition = sm.tsa.seasonal_decompose(data['ISEQ20'], model='additive')\n",
        "\n",
        "# Create a new DataFrame with decomposed components\n",
        "decomposed_data = pd.DataFrame({\n",
        "    'ISEQ20_trend': iseq20_decomposition.trend,\n",
        "    'ISEQ20_seasonal': iseq20_decomposition.seasonal,\n",
        "    'ISEQ20_residual': iseq20_decomposition.resid,\n",
        "}, index=data.index)\n",
        "\n",
        "# Create plots for trend, seasonal, and residual components\n",
        "fig_trend = px.line(decomposed_data, x=decomposed_data.index, y=['ISEQ20_trend'],\n",
        "                     title=\"Trend Component for ISEQ20\",\n",
        "                     labels={'value': 'Trend', 'index': 'Date'})\n",
        "\n",
        "fig_seasonal = px.line(decomposed_data, x=decomposed_data.index, y=['ISEQ20_seasonal'],\n",
        "                       title=\"Seasonal Component for ISEQ20\",\n",
        "                       labels={'value': 'Seasonal Component', 'index': 'Date'})\n",
        "\n",
        "fig_residual = px.line(decomposed_data, x=decomposed_data.index, y=['ISEQ20_residual'],\n",
        "                       title=\"Residual Component for ISEQ20\",\n",
        "                       labels={'value': 'Residual', 'index': 'Date'})\n",
        "\n",
        "# Show the plots\n",
        "fig_trend.show()\n",
        "fig_seasonal.show()\n",
        "fig_residual.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZW4lpU7s9bj7"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Assuming your data already has a datetime index\n",
        "data.index = pd.to_datetime(data['Date'])  # Use the 'Date' column as the index\n",
        "\n",
        "# Perform time series decomposition\n",
        "iseq20_decomposition = seasonal_decompose(data['Close'], model='additive', period=30)  # Adjust period as needed\n",
        "\n",
        "# Create subplots for observed, trend, seasonal, and residual components using Plotly\n",
        "fig = make_subplots(rows=4, cols=1, shared_xaxes=True,\n",
        "                    subplot_titles=(\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"))\n",
        "\n",
        "# Add traces for each component\n",
        "fig.add_trace(go.Scatter(x=data.index, y=data['Close'], mode='lines', name='ISEQ20'),\n",
        "              row=1, col=1)\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_decomposition.trend, mode='lines', name='Trend'),\n",
        "              row=2, col=1)\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_decomposition.seasonal, mode='lines', name='Seasonal'),\n",
        "              row=3, col=1)\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_decomposition.resid, mode='lines', name='Residual'),\n",
        "              row=4, col=1)\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title=\"Time Series Decomposition for ISEQ20\",\n",
        "                  xaxis_title=\"Date\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K-TN8_nI0qf-"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Set 'Date' column as the index\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Calculate rolling mean and standard deviation of 'Close' prices\n",
        "rolling_mean = data['Close'].rolling(window=30).mean()\n",
        "rolling_std = data['Close'].rolling(window=30).std()\n",
        "\n",
        "# Create traces\n",
        "trace1 = go.Scatter(x=data.index, y=data['Close'], mode='lines', name='Original')\n",
        "trace2 = go.Scatter(x=data.index, y=rolling_mean, mode='lines', name='Rolling Mean (30 days)')\n",
        "trace3 = go.Scatter(x=data.index, y=rolling_std, mode='lines', name='Rolling Std (30 days)')\n",
        "\n",
        "# Create layout\n",
        "layout = go.Layout(\n",
        "    title='Rolling Mean and Standard Deviation of Closing Prices',\n",
        "    xaxis=dict(title='Date'),\n",
        "    yaxis=dict(title='Price'),\n",
        "    showlegend=True,\n",
        ")\n",
        "\n",
        "# Create figure\n",
        "fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
        "\n",
        "# Show the figure\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9GhpJjcD-Qyt"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Assuming your data already has a datetime index\n",
        "data.index = pd.to_datetime(data['Date'])  # Use the 'Date' column as the index\n",
        "\n",
        "# Calculate Simple Moving Average (SMA) with a window size of 10\n",
        "sma_window = 10\n",
        "iseq20_sma = data['Close'].rolling(window=sma_window).mean()\n",
        "\n",
        "# Calculate Expanding Moving Average\n",
        "iseq20_ema = data['Close'].expanding().mean()\n",
        "\n",
        "# Calculate Exponentially Weighted Moving Average (EWMA)\n",
        "alpha = 0.2  # Smoothing factor\n",
        "iseq20_ewma = data['Close'].ewm(alpha=alpha, adjust=False).mean()\n",
        "\n",
        "# Create a plot for moving averages using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Original data\n",
        "fig.add_trace(go.Scatter(x=data.index, y=data['Close'], mode='lines', name='ISEQ20'))\n",
        "\n",
        "# Simple Moving Averages\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_sma, mode='lines', name=f'ISEQ20 SMA ({sma_window})'))\n",
        "\n",
        "# Expanding Moving Averages\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_ema, mode='lines', name='ISEQ20 Expanding MA'))\n",
        "\n",
        "# Exponentially Weighted Moving Averages\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_ewma, mode='lines', name=f'ISEQ20 EWMA (alpha={alpha})'))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title=\"Moving Averages\",\n",
        "                  xaxis_title=\"Date\",\n",
        "                  yaxis_title=\"Value\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tMxrEOkD-tUz"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Assuming your data already has a datetime index\n",
        "data.index = pd.to_datetime(data['Date'])  # Use the 'Date' column as the index\n",
        "\n",
        "# Fit Double Exponential Smoothing model for ISEQ20\n",
        "iseq20_model = ExponentialSmoothing(data['Close'], trend='add', seasonal='add', seasonal_periods=7)\n",
        "iseq20_fit = iseq20_model.fit()\n",
        "\n",
        "# Forecast for the next 7 days\n",
        "forecast_steps = 7\n",
        "iseq20_forecast = iseq20_fit.forecast(steps=forecast_steps)\n",
        "\n",
        "# Create a plot for Double Exponential Smoothing using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Original data\n",
        "fig.add_trace(go.Scatter(x=data.index, y=data['Close'], mode='lines', name='ISEQ20'))\n",
        "\n",
        "# Fitted values\n",
        "fig.add_trace(go.Scatter(x=data.index, y=iseq20_fit.fittedvalues, mode='lines', name='ISEQ20 Fitted'))\n",
        "\n",
        "# Forecasted values\n",
        "forecast_index = pd.date_range(start=data.index[-1], periods=forecast_steps + 1, freq='D')[1:]  # Start from the next day\n",
        "fig.add_trace(go.Scatter(x=forecast_index, y=iseq20_forecast, mode='lines', name='ISEQ20 Forecast'))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title=\"Double Exponential Smoothing\",\n",
        "                  xaxis_title=\"Date\",\n",
        "                  yaxis_title=\"Value\")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voJiBEDSlS2O"
      },
      "source": [
        "# **prophet model - times series**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "arJibFZgnr7r"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel('Cleaned_ISEQ20.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VFU6ClOsuiX2"
      },
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Preprocess the data and create the Prophet model\n",
        "df = df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "model = Prophet()\n",
        "model.fit(df)\n",
        "\n",
        "# Create a DataFrame for future dates and generate forecasts\n",
        "future = model.make_future_dataframe(periods=6 * 30)  # 6 months forecast, assuming 30 days per month\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Create a detailed Plotly figure for the forecast\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add actual values trace\n",
        "fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='lines', name='Actual', line=dict(color='blue')))\n",
        "# Add forecasted values trace\n",
        "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast', line=dict(color='orange')))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Actual and Forecasted Close Values',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Close Value',\n",
        "    hovermode='x',  # Show hover information only for the closest data point along the x-axis\n",
        "    template='plotly',  # Use the \"plotly\" template for a cleaner look\n",
        "    legend=dict(x=0.02, y=0.98),  # Position the legend\n",
        "    margin=dict(l=0, r=0, t=50, b=0)  # Add margin for the title\n",
        ")\n",
        "\n",
        "# Add hover text\n",
        "hover_text = [\"Actual: {:.2f}\".format(y) if idx < len(df) else \"Forecast: {:.2f}\".format(y)\n",
        "              for idx, y in enumerate(list(df['y']) + list(forecast['yhat']))]\n",
        "fig.update_traces(text=hover_text, hoverinfo='text+y')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxBZ52YW_arc"
      },
      "source": [
        "# Confirm to use hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3c9-7J4RyEYB"
      },
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Preprocess the data and create the Prophet model\n",
        "df = df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "model = Prophet()\n",
        "model.fit(df)\n",
        "\n",
        "# Create a DataFrame for future dates and generate forecasts\n",
        "future = model.make_future_dataframe(periods=6 * 30)  # 6 months forecast, assuming 30 days per month\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Create a detailed Plotly figure for the forecast with confidence intervals\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add actual values trace\n",
        "fig.add_trace(go.Scatter(x=df['ds'], y=df['y'], mode='lines', name='Actual', line=dict(color='blue')))\n",
        "\n",
        "# Add forecasted values trace\n",
        "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast', line=dict(color='orange')))\n",
        "\n",
        "# Add upper and lower confidence interval traces\n",
        "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill=None, mode='lines', line=dict(color='rgba(255, 165, 0, 0.3)'), name='Upper Confidence Interval'))\n",
        "fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='lines', line=dict(color='rgba(255, 165, 0, 0.3)'), name='Lower Confidence Interval'))\n",
        "\n",
        "# Customize layout\n",
        "fig.update_layout(\n",
        "    title='Actual and Forecasted Close Values with Confidence Intervals',\n",
        "    xaxis_title='Date',\n",
        "    yaxis_title='Close Value',\n",
        "    hovermode='x',  # Show hover information only for the closest data point along the x-axis\n",
        "    template='plotly',  # Use the \"plotly\" template for a cleaner look\n",
        "    legend=dict(x=0.02, y=0.98),  # Position the legend\n",
        "    margin=dict(l=0, r=0, t=50, b=0)  # Add margin for the title\n",
        ")\n",
        "\n",
        "# Add hover text\n",
        "hover_text = [\"Actual: {:.2f}\".format(y) if idx < len(df) else \"Forecast: {:.2f}\".format(y)\n",
        "              for idx, y in enumerate(list(df['y']) + list(forecast['yhat']))]\n",
        "fig.update_traces(text=hover_text, hoverinfo='text+y')\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKnQbKT_v55L"
      },
      "source": [
        "# performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "92Z2aCyxvtrz"
      },
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Preprocess the data and create the Prophet model\n",
        "df = df.rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "model = Prophet()\n",
        "model.fit(df)\n",
        "\n",
        "# Create a DataFrame for future dates and generate forecasts\n",
        "future = model.make_future_dataframe(periods=6 * 30)  # 6 months forecast, assuming 30 days per month\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Calculate Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(df['y'], forecast['yhat'][:len(df)])\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(df['y'], forecast['yhat'][:len(df)])\n",
        "\n",
        "# Calculate Root Mean Squared Error (RMSE)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Calculate Mean Absolute Percentage Error (MAPE)\n",
        "mape = np.mean(np.abs((df['y'] - forecast['yhat'][:len(df)]) / df['y'])) * 100\n",
        "\n",
        "# Calculate R-squared (R2)\n",
        "y_actual = df['y']\n",
        "y_forecasted = forecast['yhat'][:len(df)]\n",
        "y_mean = np.mean(y_actual)\n",
        "ss_residual = np.sum((y_actual - y_forecasted) ** 2)\n",
        "ss_total = np.sum((y_actual - y_mean) ** 2)\n",
        "r2 = 1 - (ss_residual / ss_total)\n",
        "\n",
        "# Create a table comparing performance metrics\n",
        "performance_metrics = pd.DataFrame({\n",
        "    'Metric': ['Mean Absolute Error (MAE)', 'Mean Squared Error (MSE)', 'Root Mean Squared Error (RMSE)',\n",
        "               'Mean Absolute Percentage Error (MAPE)', 'R-squared (R2)'],\n",
        "    'Value': [mae, mse, rmse, mape, r2]\n",
        "})\n",
        "\n",
        "print(performance_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1gpsB_XhGcD"
      },
      "source": [
        "# Prophet model compared to other algoritmns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "roJyX8lj8qr3"
      },
      "outputs": [],
      "source": [
        "# Replace the following line with the actual path to your Excel file\n",
        "file_path = 'ISEQ20.xlsx'\n",
        "\n",
        "# Read the Excel file into a pandas DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Keep only 'Date' and 'Close' columns and rename them to 'ds' and 'y'\n",
        "df = df[['Date', 'Close']]\n",
        "df.columns = ['ds', 'y']\n",
        "\n",
        "# Sort the data based on 'Date' column\n",
        "df.sort_values('ds', inplace=True)\n",
        "\n",
        "# Function to calculate performance metrics for each algorithm\n",
        "def calculate_metrics(actual, forecast):\n",
        "    mae = mean_absolute_error(actual, forecast)\n",
        "    mse = mean_squared_error(actual, forecast)\n",
        "    rmse = mean_squared_error(actual, forecast, squared=False)\n",
        "    mape = mean_absolute_percentage_error(actual, forecast)\n",
        "    r2 = r2_score(actual, forecast)\n",
        "    return mae, mse, rmse, mape, r2\n",
        "\n",
        "# ARIMA\n",
        "def run_arima():\n",
        "    model = ARIMA(df['y'], order=(5, 1, 0))\n",
        "    model_fit = model.fit()\n",
        "    forecast = model_fit.forecast(steps=len(df))\n",
        "    return forecast\n",
        "\n",
        "# SARIMA\n",
        "def run_sarima():\n",
        "    model = SARIMAX(df['y'], order=(5, 1, 0), seasonal_order=(1, 1, 1, 12))\n",
        "    model_fit = model.fit()\n",
        "    forecast = model_fit.forecast(steps=len(df))\n",
        "    return forecast\n",
        "\n",
        "# LSTM\n",
        "def run_lstm():\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df['y'].values.reshape(-1, 1))\n",
        "\n",
        "    # Create sequences and labels\n",
        "    sequence_length = 10\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        sequences.append(scaled_data[i:i+sequence_length])\n",
        "        labels.append(scaled_data[i+sequence_length])\n",
        "    sequences, labels = np.array(sequences), np.array(labels)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Build LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, return_sequences=True, input_shape=(sequence_length, 1)))\n",
        "    model.add(LSTM(50, return_sequences=False))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, batch_size=1, epochs=5)\n",
        "\n",
        "    # Make predictions\n",
        "    lstm_forecast = []\n",
        "    last_sequence = X_train[-1].reshape(1, sequence_length, 1)\n",
        "    for i in range(len(X_test)):\n",
        "        next_pred = model.predict(last_sequence)\n",
        "        lstm_forecast.append(next_pred[0, 0])\n",
        "        last_sequence = np.append(last_sequence[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
        "\n",
        "    lstm_forecast = scaler.inverse_transform(np.array(lstm_forecast).reshape(-1, 1)).flatten()\n",
        "    return lstm_forecast\n",
        "\n",
        "# Neural Prophet\n",
        "def run_neural_prophet():\n",
        "    model = NeuralProphet()\n",
        "    model.fit(df, freq='D')\n",
        "    future = model.make_future_dataframe(df, periods=len(df))\n",
        "    forecast = model.predict(future)['yhat1'].tail(len(df))\n",
        "    return forecast\n",
        "\n",
        "# Prophet\n",
        "def run_prophet():\n",
        "    model = Prophet()\n",
        "    df_prophet = df.rename(columns={'ds': 'ds', 'y': 'y'})\n",
        "    model.fit(df_prophet)\n",
        "    future = model.make_future_dataframe(periods=len(df))\n",
        "    forecast = model.predict(future)\n",
        "    return forecast['yhat'].tail(len(df))\n",
        "\n",
        "# GARCH\n",
        "def run_garch():\n",
        "    model = arch_model(df['y'], vol='Garch', p=1, q=1)\n",
        "    model_fit = model.fit(disp='off')\n",
        "    forecast = np.sqrt(model_fit.conditional_volatility.tail(len(df)))\n",
        "    return forecast\n",
        "\n",
        "# Run each algorithm and calculate metrics\n",
        "arima_forecast = run_arima()\n",
        "sarima_forecast = run_sarima()\n",
        "lstm_forecast = run_lstm()\n",
        "neural_prophet_forecast = run_neural_prophet()\n",
        "prophet_forecast = run_prophet()\n",
        "garch_forecast = run_garch()\n",
        "\n",
        "# Calculate performance metrics\n",
        "actual = df['y']\n",
        "arima_metrics = calculate_metrics(actual, arima_forecast)\n",
        "sarima_metrics = calculate_metrics(actual, sarima_forecast)\n",
        "lstm_metrics = calculate_metrics(actual[-len(lstm_forecast):], lstm_forecast)\n",
        "neural_prophet_metrics = calculate_metrics(actual, neural_prophet_forecast)\n",
        "prophet_metrics = calculate_metrics(actual, prophet_forecast)\n",
        "garch_metrics = calculate_metrics(actual, garch_forecast)\n",
        "\n",
        "# Create a comparison table\n",
        "comparison_table = pd.DataFrame({\n",
        "    'Algorithm': ['ARIMA', 'SARIMA', 'LSTM', 'NeuralProphet', 'Prophet', 'GARCH'],\n",
        "    'MAE': [arima_metrics[0], sarima_metrics[0], lstm_metrics[0], neural_prophet_metrics[0], prophet_metrics[0], garch_metrics[0]],\n",
        "    'MSE': [arima_metrics[1], sarima_metrics[1], lstm_metrics[1], neural_prophet_metrics[1], prophet_metrics[1], garch_metrics[1]],\n",
        "    'RMSE': [arima_metrics[2], sarima_metrics[2], lstm_metrics[2], neural_prophet_metrics[2], prophet_metrics[2], garch_metrics[2]],\n",
        "    'MAPE': [arima_metrics[3], sarima_metrics[3], lstm_metrics[3], neural_prophet_metrics[3], prophet_metrics[3], garch_metrics[3]],\n",
        "    'R2': [arima_metrics[4], sarima_metrics[4], lstm_metrics[4], neural_prophet_metrics[4], prophet_metrics[4], garch_metrics[4]]\n",
        "})\n",
        "\n",
        "print(comparison_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oj_Q0u5y-ebP"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot for each metric\n",
        "mae_plot = px.bar(comparison_table, x='Algorithm', y='MAE', title='Mean Absolute Error (MAE)')\n",
        "mse_plot = px.bar(comparison_table, x='Algorithm', y='MSE', title='Mean Squared Error (MSE)')\n",
        "rmse_plot = px.bar(comparison_table, x='Algorithm', y='RMSE', title='Root Mean Squared Error (RMSE)')\n",
        "mape_plot = px.bar(comparison_table, x='Algorithm', y='MAPE', title='Mean Absolute Percentage Error (MAPE)')\n",
        "r2_plot = px.bar(comparison_table, x='Algorithm', y='R2', title='R-squared (R2)')\n",
        "\n",
        "# Create a subplot for all metrics\n",
        "from plotly.subplots import make_subplots\n",
        "fig_metrics = make_subplots(rows=3, cols=2, subplot_titles=('MAE', 'MSE', 'RMSE', 'MAPE', 'R2'))\n",
        "fig_metrics.add_trace(mae_plot.data[0], row=1, col=1)\n",
        "fig_metrics.add_trace(mse_plot.data[0], row=1, col=2)\n",
        "fig_metrics.add_trace(rmse_plot.data[0], row=2, col=1)\n",
        "fig_metrics.add_trace(mape_plot.data[0], row=2, col=2)\n",
        "fig_metrics.add_trace(r2_plot.data[0], row=3, col=1)\n",
        "\n",
        "# Update layout of the subplot\n",
        "fig_metrics.update_layout(title='Performance Metrics Comparison', showlegend=False)\n",
        "\n",
        "# Create a bar plot for all metrics combined\n",
        "fig_combined_metrics = px.bar(comparison_table, x='Algorithm', y=['MAE', 'MSE', 'RMSE', 'MAPE', 'R2'],\n",
        "                               title='Combined Metrics Comparison')\n",
        "\n",
        "# Show the plots\n",
        "fig_metrics.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pYGKqsZqAFa-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a heatmap to visualize the metrics across models\n",
        "fig_heatmap_metrics = px.imshow(\n",
        "    comparison_table.set_index('Algorithm'),\n",
        "    title='Heatmap of Metrics by Model'\n",
        ")\n",
        "\n",
        "# Show the plots\n",
        "\n",
        "fig_heatmap_metrics.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1CMNQDC4pAx"
      },
      "source": [
        "### **Anomaly detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IsxqltWyzI03"
      },
      "outputs": [],
      "source": [
        "# Read the dataset\n",
        "df = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Calculate Z-scores for closing prices\n",
        "df['Z_score'] = (df['Close'] - df['Close'].rolling(window=30).mean()) / df['Close'].rolling(window=30).std()\n",
        "\n",
        "# Define a threshold for anomaly detection\n",
        "threshold = 2  # Adjust as needed\n",
        "\n",
        "# Identify anomalies based on the Z-score threshold\n",
        "df['Anomaly'] = np.where(df['Z_score'].abs() > threshold, True, False)\n",
        "\n",
        "# Plot the closing prices with anomalies highlighted\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Close'], label='Closing Price', color='blue')\n",
        "plt.scatter(df[df['Anomaly']]['Date'], df[df['Anomaly']]['Close'], color='red', label='Anomalies')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Prices with Anomalies')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IVo1zeGuIGvJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data[['Date', 'Close']]\n",
        "data.columns = ['ds', 'y']\n",
        "\n",
        "# Initialize and fit the Prophet model\n",
        "model = Prophet(daily_seasonality=True)\n",
        "model.fit(data)\n",
        "\n",
        "# Make future predictions\n",
        "future = model.make_future_dataframe(periods=0)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Calculate residuals\n",
        "data['residual'] = data['y'] - forecast['yhat']\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residual_std = data['residual'].std()\n",
        "\n",
        "# Define a threshold for anomaly detection\n",
        "threshold = 2.5\n",
        "\n",
        "# Identify anomalies\n",
        "data['anomaly'] = np.abs(data['residual']) > threshold * residual_std\n",
        "\n",
        "# Create interactive plots using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Actual and predicted close prices\n",
        "fig.add_trace(go.Scatter(x=data['ds'], y=data['y'], mode='lines', name='Actual'))\n",
        "fig.add_trace(go.Scatter(x=data['ds'], y=forecast['yhat'], mode='lines', name='Predicted'))\n",
        "fig.add_trace(go.Scatter(x=data[data['anomaly']]['ds'], y=data[data['anomaly']]['y'], mode='markers', name='Anomalies', marker=dict(color='red')))\n",
        "fig.update_layout(title='Actual vs. Predicted Close Prices with Anomalies',\n",
        "                  xaxis_title='Date', yaxis_title='Close Price')\n",
        "fig.show()\n",
        "\n",
        "# Residuals and anomalies\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=data['residual'], mode='lines', name='Residuals'))\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=[threshold * residual_std] * len(data), mode='lines', name='Anomaly Threshold', line=dict(dash='dash')))\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=[-threshold * residual_std] * len(data), mode='lines', showlegend=False, line=dict(dash='dash')))\n",
        "fig2.add_trace(go.Scatter(x=data[data['anomaly']]['ds'], y=data[data['anomaly']]['residual'], mode='markers', name='Anomalies', marker=dict(color='red')))\n",
        "fig2.update_layout(title='Residuals and Anomalies',\n",
        "                   xaxis_title='Date', yaxis_title='Residual',\n",
        "                   shapes=[{'type': 'line', 'x0': data['ds'].iloc[0], 'x1': data['ds'].iloc[-1], 'y0': threshold * residual_std, 'y1': threshold * residual_std,\n",
        "                            'line': {'color': 'red', 'width': 2, 'dash': 'dash'}}])\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk5Fw8ihzxC2"
      },
      "source": [
        "# comparision with other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PISxj4y6KlGf"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data[['Date', 'Close']]\n",
        "data.columns = ['ds', 'y']\n",
        "\n",
        "# Initialize and fit the Prophet model\n",
        "model = Prophet(daily_seasonality=True)\n",
        "model.fit(data)\n",
        "\n",
        "# Make future predictions\n",
        "future = model.make_future_dataframe(periods=0)\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Calculate residuals\n",
        "data['residual'] = data['y'] - forecast['yhat']\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residual_std = data['residual'].std()\n",
        "\n",
        "# Define a threshold for anomaly detection\n",
        "threshold = 2.5\n",
        "\n",
        "# Identify anomalies\n",
        "data['anomaly'] = np.abs(data['residual']) > threshold * residual_std\n",
        "\n",
        "# Create interactive plots using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "# Actual and predicted close prices\n",
        "fig.add_trace(go.Scatter(x=data['ds'], y=data['y'], mode='lines', name='Actual'))\n",
        "fig.add_trace(go.Scatter(x=data['ds'], y=forecast['yhat'], mode='lines', name='Predicted'))\n",
        "fig.add_trace(go.Scatter(x=data[data['anomaly']]['ds'], y=data[data['anomaly']]['y'], mode='markers', name='Anomalies', marker=dict(color='red')))\n",
        "fig.update_layout(title='Actual vs. Predicted Close Prices with Anomalies',\n",
        "                  xaxis_title='Date', yaxis_title='Close Price')\n",
        "fig.show()\n",
        "\n",
        "# Residuals and anomalies\n",
        "fig2 = go.Figure()\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=data['residual'], mode='lines', name='Residuals'))\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=[threshold * residual_std] * len(data), mode='lines', name='Anomaly Threshold', line=dict(dash='dash')))\n",
        "fig2.add_trace(go.Scatter(x=data['ds'], y=[-threshold * residual_std] * len(data), mode='lines', showlegend=False, line=dict(dash='dash')))\n",
        "fig2.add_trace(go.Scatter(x=data[data['anomaly']]['ds'], y=data[data['anomaly']]['residual'], mode='markers', name='Anomalies', marker=dict(color='red')))\n",
        "fig2.update_layout(title='Residuals and Anomalies',\n",
        "                   xaxis_title='Date', yaxis_title='Residual',\n",
        "                   shapes=[{'type': 'line', 'x0': data['ds'].iloc[0], 'x1': data['ds'].iloc[-1], 'y0': threshold * residual_std, 'y1': threshold * residual_std,\n",
        "                            'line': {'color': 'red', 'width': 2, 'dash': 'dash'}}])\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pX62AJqv8mn7"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(\"Cleaned_ISEQ20.xlsx\")\n",
        "\n",
        "# Prepare the data\n",
        "X = data[[\"Close\"]].values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.01)\n",
        "iso_forest.fit(X_train)\n",
        "iso_preds = iso_forest.predict(X_test)\n",
        "\n",
        "# One-Class SVM\n",
        "svm = OneClassSVM(nu=0.01)\n",
        "svm.fit(X_train)\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "# Prophet\n",
        "data_prophet = data.rename(columns={\"Date\": \"ds\", \"Close\": \"y\"})[['ds', 'y']]\n",
        "prophet = Prophet()\n",
        "prophet.fit(data_prophet)\n",
        "future = prophet.make_future_dataframe(periods=len(X_test))\n",
        "prophet_forecast = prophet.predict(future)\n",
        "prophet_preds_all = prophet_forecast[\"yhat\"].values\n",
        "prophet_preds = prophet_preds_all[-len(X_test):]\n",
        "\n",
        "# Support Vector Machine (SVM)\n",
        "svm = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm.fit(X_train, np.sign(X_train))\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    roc_auc = None\n",
        "    if len(np.unique(y_true)) == 2:\n",
        "        roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred)\n",
        "    pr_auc = auc(recall_vals, precision_vals)\n",
        "\n",
        "    return precision, recall, f1, roc_auc, pr_auc\n",
        "\n",
        "iso_metrics = calculate_metrics(np.ones_like(iso_preds), iso_preds)\n",
        "svm_metrics = calculate_metrics(np.ones_like(svm_preds), svm_preds)\n",
        "prophet_metrics = calculate_metrics(np.sign(X_test), np.sign(prophet_preds))\n",
        "\n",
        "# Create a comparison table\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Model': ['Isolation Forest', 'One-Class SVM', 'Prophet', 'Support Vector Machine'],\n",
        "    'Precision': [iso_metrics[0], svm_metrics[0], prophet_metrics[0], svm_metrics[0]],\n",
        "    'Recall': [iso_metrics[1], svm_metrics[1], prophet_metrics[1], svm_metrics[1]],\n",
        "    'F1-Score': [iso_metrics[2], svm_metrics[2], prophet_metrics[2], svm_metrics[2]],\n",
        "    'AUC-ROC': [iso_metrics[3], svm_metrics[3], prophet_metrics[3], svm_metrics[3]],\n",
        "    'AUC-PR': [iso_metrics[4], svm_metrics[4], prophet_metrics[4], svm_metrics[4]]\n",
        "})\n",
        "\n",
        "print(metrics_table)\n",
        "\n",
        "# Plot Precision-Recall curves\n",
        "precision_vals, recall_vals, _ = precision_recall_curve(np.sign(X_test), np.sign(prophet_preds))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.step(recall_vals, precision_vals, color='b', alpha=0.5, where='post')\n",
        "plt.fill_between(recall_vals, precision_vals, alpha=0.2, color='b')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TWbBiOJILMNm"
      },
      "outputs": [],
      "source": [
        "# Plot performance metrics\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Precision plot\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.bar(metrics_table['Model'], metrics_table['Precision'], color='blue')\n",
        "plt.title('Precision')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# Recall plot\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.bar(metrics_table['Model'], metrics_table['Recall'], color='green')\n",
        "plt.title('Recall')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# F1-Score plot\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.bar(metrics_table['Model'], metrics_table['F1-Score'], color='orange')\n",
        "plt.title('F1-Score')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# AUC-ROC plot\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.bar(metrics_table['Model'], metrics_table['AUC-ROC'], color='purple')\n",
        "plt.title('AUC-ROC')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "# AUC-PR plot\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.bar(metrics_table['Model'], metrics_table['AUC-PR'], color='red')\n",
        "plt.title('AUC-PR')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7_M9FxhJLOgF"
      },
      "outputs": [],
      "source": [
        "# Create a Plotly bar plot for each performance metric\n",
        "fig = px.bar(metrics_table, x='Model', y=['Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR'],\n",
        "             title='Performance Metrics Comparison for Different Models',\n",
        "             labels={'value': 'Score', 'variable': 'Metric'},\n",
        "             template='plotly_dark')\n",
        "\n",
        "# Customize the layout\n",
        "fig.update_layout(\n",
        "    barmode='group',\n",
        "    legend=dict(orientation='h', yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1, font=dict(size=10)),\n",
        "    height=600,\n",
        "    xaxis=dict(title='Model', showline=True, showgrid=False, tickangle=45),\n",
        "    yaxis=dict(title='Score', showline=True, showgrid=True),\n",
        "    title_font=dict(size=18),\n",
        "    margin=dict(t=80),  # Increase top margin for better title visibility\n",
        ")\n",
        "\n",
        "# Adjust bar colors for better differentiation\n",
        "color_palette = px.colors.qualitative.Set1\n",
        "for i, metric in enumerate(['Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR']):\n",
        "    fig.data[i].marker.color = color_palette[i]\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PZ5ornCW68Y2"
      },
      "outputs": [],
      "source": [
        "#pip install bayesian-optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-w1RwcPj725m"
      },
      "outputs": [],
      "source": [
        "#pip install --upgrade pandas numpy scikit-learn prophet neuralprophet statsmodels bayesian-optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "olU3N24d8JGE"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall torchaudio\n",
        "#!pip install torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zjsw1UF88Orf"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall neuralprophet\n",
        "#!pip install neuralprophet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "caPOER2A68us"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('ISEQ20.xlsx')\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data.set_index('Date', inplace=True)\n",
        "\n",
        "# Define a function to optimize Isolation Forest\n",
        "def optimize_isolation_forest(n_estimators, contamination):\n",
        "    model = IsolationForest(n_estimators=int(n_estimators), contamination=contamination, random_state=42)\n",
        "    model.fit(data[['Close']])\n",
        "    y_pred = model.predict(data[['Close']])\n",
        "    precision = precision_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "    recall = recall_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "    f1 = f1_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "    return -f1  # We use negative f1-score because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Define a function to optimize Prophet\n",
        "def optimize_prophet(changepoint_prior_scale, seasonality_prior_scale):\n",
        "    model = Prophet(\n",
        "        changepoint_prior_scale=changepoint_prior_scale,\n",
        "        seasonality_prior_scale=seasonality_prior_scale,\n",
        "    )\n",
        "    data_prophet = data.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "    model.fit(data_prophet)\n",
        "    future = model.make_future_dataframe(periods=1)\n",
        "    forecast = model.predict(future)\n",
        "    y_pred = forecast['yhat'].values[-1]\n",
        "    return -y_pred  # We use negative predicted value because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Define a function to optimize NeuralProphet\n",
        "def optimize_neural_prophet(n_forecasts, num_hidden_layers):\n",
        "    model = NeuralProphet(\n",
        "        n_forecasts=int(n_forecasts),\n",
        "        n_lags=10,\n",
        "        num_hidden_layers=int(num_hidden_layers),\n",
        "        yearly_seasonality=False,\n",
        "        weekly_seasonality=False,\n",
        "    )\n",
        "    data_neuralprophet = data.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "    model.fit(data_neuralprophet, freq='D')\n",
        "    future = model.make_future_dataframe(data_neuralprophet, periods=1)\n",
        "    forecast = model.predict(future)\n",
        "    y_pred = forecast['yhat1'].values[-1]\n",
        "    return -y_pred  # We use negative predicted value because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Define a function to optimize SARIMA\n",
        "def optimize_sarima(p, d, q, P, D, Q, s):\n",
        "    model = SARIMAX(data['Close'], order=(int(p), int(d), int(q)), seasonal_order=(int(P), int(D), int(Q), int(s)))\n",
        "    results = model.fit()\n",
        "    forecast = results.get_forecast(steps=1)\n",
        "    y_pred = forecast.predicted_mean.values[0]\n",
        "    return -y_pred  # We use negative predicted value because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Define a function to optimize ARIMA\n",
        "def optimize_arima(p, d, q):\n",
        "    model = ARIMA(data['Close'], order=(int(p), int(d), int(q)))\n",
        "    results = model.fit()\n",
        "    forecast = results.get_forecast(steps=1)\n",
        "    y_pred = forecast.predicted_mean.values[0]\n",
        "    return -y_pred  # We use negative predicted value because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Define a function to optimize GRU\n",
        "def optimize_gru(n_units, n_layers, batch_size):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - 1):\n",
        "        X.append(data['Close'].iloc[i:i+10].values)\n",
        "        y.append(data['Close'].iloc[i+1])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(GRU(int(n_units), input_shape=(10, 1), return_sequences=True))\n",
        "    for _ in range(int(n_layers) - 1):\n",
        "        model.add(GRU(int(n_units), return_sequences=True))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=5, batch_size=int(batch_size), verbose=0)\n",
        "    y_pred = model.predict(X_test)[-1][0]\n",
        "    return -y_pred  # We use negative predicted value because Bayesian Optimization tries to maximize\n",
        "\n",
        "# Bayesian Optimization for Isolation Forest hyperparameters\n",
        "isolation_forest_optimizer = BayesianOptimization(\n",
        "    f=optimize_isolation_forest,\n",
        "    pbounds={'n_estimators': (10, 100), 'contamination': (0.01, 0.1)}\n",
        ")\n",
        "isolation_forest_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Bayesian Optimization for Prophet hyperparameters\n",
        "prophet_optimizer = BayesianOptimization(\n",
        "    f=optimize_prophet,\n",
        "    pbounds={'changepoint_prior_scale': (0.001, 1.0), 'seasonality_prior_scale': (0.01, 10)}\n",
        ")\n",
        "prophet_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Bayesian Optimization for NeuralProphet hyperparameters\n",
        "neural_prophet_optimizer = BayesianOptimization(\n",
        "    f=optimize_neural_prophet,\n",
        "    pbounds={'n_forecasts': (1, 100), 'num_hidden_layers': (1, 10)}\n",
        ")\n",
        "neural_prophet_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Bayesian Optimization for SARIMA hyperparameters\n",
        "sarima_optimizer = BayesianOptimization(\n",
        "    f=optimize_sarima,\n",
        "    pbounds={'p': (0, 5), 'd': (0, 2), 'q': (0, 5), 'P': (0, 5), 'D': (0, 2), 'Q': (0, 5), 's': (1, 365)}\n",
        ")\n",
        "sarima_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Bayesian Optimization for ARIMA hyperparameters\n",
        "arima_optimizer = BayesianOptimization(\n",
        "    f=optimize_arima,\n",
        "    pbounds={'p': (0, 5), 'd': (0, 2), 'q': (0, 5)}\n",
        ")\n",
        "arima_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Bayesian Optimization for GRU hyperparameters\n",
        "gru_optimizer = BayesianOptimization(\n",
        "    f=optimize_gru,\n",
        "    pbounds={'n_units': (10, 100), 'n_layers': (1, 5), 'batch_size': (1, 64)}\n",
        ")\n",
        "gru_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Extract best hyperparameters from each optimization\n",
        "best_hyperparameters = {\n",
        "    'Isolation Forest': isolation_forest_optimizer.max,\n",
        "    'Prophet': prophet_optimizer.max,\n",
        "    'NeuralProphet': neural_prophet_optimizer.max,\n",
        "    'SARIMA': sarima_optimizer.max,\n",
        "    'ARIMA': arima_optimizer.max,\n",
        "    'GRU': gru_optimizer.max,\n",
        "}\n",
        "\n",
        "# Create a table comparing models and metrics\n",
        "metrics_table = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1-Score'])\n",
        "for model_name, hyperparameters in best_hyperparameters.items():\n",
        "    if model_name == 'Isolation Forest':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        model = IsolationForest(n_estimators=int(hyperparams['n_estimators']), contamination=hyperparams['contamination'], random_state=42)\n",
        "        model.fit(data[['Close']])\n",
        "        y_pred = model.predict(data[['Close']])\n",
        "    elif model_name == 'Prophet':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        model = Prophet(changepoint_prior_scale=hyperparams['changepoint_prior_scale'], seasonality_prior_scale=hyperparams['seasonality_prior_scale'])\n",
        "        data_prophet = data.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "        model.fit(data_prophet)\n",
        "        future = model.make_future_dataframe(periods=1)\n",
        "        forecast = model.predict(future)\n",
        "        y_pred = forecast['yhat'].values[-1]\n",
        "    elif model_name == 'NeuralProphet':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        model = NeuralProphet(\n",
        "            n_forecasts=int(hyperparams['n_forecasts']),\n",
        "            n_lags=10,\n",
        "            num_hidden_layers=int(hyperparams['num_hidden_layers']),\n",
        "            yearly_seasonality=False,\n",
        "            weekly_seasonality=False,\n",
        "        )\n",
        "        data_neuralprophet = data.reset_index().rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "        model.fit(data_neuralprophet, freq='D')\n",
        "        future = model.make_future_dataframe(data_neuralprophet, periods=1)\n",
        "        forecast = model.predict(future)\n",
        "        y_pred = forecast['yhat1'].values[-1]\n",
        "    elif model_name == 'SARIMA':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        model = SARIMAX(data['Close'], order=(int(hyperparams['p']), int(hyperparams['d']), int(hyperparams['q'])), seasonal_order=(int(hyperparams['P']), int(hyperparams['D']), int(hyperparams['Q']), int(hyperparams['s'])))\n",
        "        results = model.fit()\n",
        "        forecast = results.get_forecast(steps=1)\n",
        "        y_pred = forecast.predicted_mean.values[0]\n",
        "    elif model_name == 'ARIMA':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        model = ARIMA(data['Close'], order=(int(hyperparams['p']), int(hyperparams['d']), int(hyperparams['q'])))\n",
        "        results = model.fit()\n",
        "        forecast = results.get_forecast(steps=1)\n",
        "        y_pred = forecast.predicted_mean.values[0]\n",
        "    elif model_name == 'GRU':\n",
        "        hyperparams = hyperparameters['params']\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - 1):\n",
        "            X.append(data['Close'].iloc[i:i+10].values)\n",
        "            y.append(data['Close'].iloc[i+1])\n",
        "        X, y = np.array(X), np.array(y)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(GRU(int(hyperparams['n_units']), input_shape=(10, 1), return_sequences=True))\n",
        "        for _ in range(int(hyperparams['n_layers']) - 1):\n",
        "            model.add(GRU(int(hyperparams['n_units']), return_sequences=True))\n",
        "        model.add(Dense(1))\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        model.fit(X_train, y_train, epochs=5, batch_size=int(hyperparams['batch_size']), verbose=0)\n",
        "        y_pred = model.predict(X_test)[-1][0]\n",
        "\n",
        "    precision = precision_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "    recall = recall_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "    f1 = f1_score(y_true=np.ones(len(data)), y_pred=y_pred, pos_label=-1)\n",
        "\n",
        "    metrics_table = metrics_table.append({'Model': model_name, 'Precision': precision, 'Recall': recall, 'F1-Score': f1}, ignore_index=True)\n",
        "\n",
        "# Print the metrics table\n",
        "print(metrics_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XY6bXnSn8Kbe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEBna0f8SCDxDG+hYICyVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}